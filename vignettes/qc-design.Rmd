---
title: "Quality Control and Design Principles in prepkit"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
vignette: >
  %\VignetteIndexEntry{Quality Control and Design Principles in prepkit}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
author: "prepkit developers"
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

Quality control (QC) is a foundational yet often implicit component of data preprocessing. In many analytical workflows, QC is embedded informally within preprocessing steps or applied in an ad-hoc manner without a clearly defined structure.

As data volume, dimensionality, and heterogeneity increase, this implicit treatment of QC becomes increasingly fragile. Decisions about data inclusion, exclusion, or modification are often made implicitly, without explicit documentation of the criteria or assumptions involved.

In *prepkit*, quality control is treated as an explicit and structured process that precedes analysis-level preprocessing. QC is not intended to improve data quality or to optimize data for downstream models. Instead, its purpose is to assess whether data are sufficiently reliable to justify further processing and analysis.

This vignette documents the design principles underlying QC in *prepkit*. It focuses on conceptual boundaries, processing order, and responsibility separation, rather than on implementation details or step-by-step usage instructions. The goal is to make QC assumptions explicit, auditable, and reproducible across datasets and projects.

## Scope and Non-Goals

This section defines the conceptual scope of quality control (QC) in *prepkit* and explicitly states what is intentionally excluded. Clear boundaries are essential to prevent QC from being conflated with data cleaning, optimization, or model-driven tuning.

### In scope

Within *prepkit*, QC is defined as a structured process that:

- Assesses whether data are suitable for downstream processing and analysis
- Produces explicit, inspectable metadata rather than modified data values
- Operates upstream of analysis-level preprocessing
- Separates assessment from decision-making
- Preserves responsibility transparency between the package and the user

QC outputs are intended to support explicit decisions such as filtering, weighting, or stratification, but do not enforce those decisions automatically.

### Non-goals

The following are intentionally out of scope for QC in *prepkit*:

- Automatic data correction, repair, or enhancement
- Implicit removal, replacement, or modification of observations
- Device-, vendor-, or protocol-specific heuristics
- Model-driven or outcome-dependent quality assessment
- Optimization of data for predictive performance
- Performance tuning, parallelization, or hardware acceleration

In particular, *prepkit* does not attempt to "fix" data or to improve data quality. Any transformation that alters data values for the purpose of analysis is considered analysis-level preprocessing and must occur after QC decisions.

By explicitly declaring these non-goals, *prepkit* aims to provide a QC framework that is predictable, auditable, and resistant to misuse.

## The Role of Quality Control in Data Preprocessing

In *prepkit*, quality control (QC) occupies a distinct methodological position that is deliberately separated from analysis-level preprocessing. This separation is essential to avoid conflating data assessment with data transformation.

Preprocessing steps such as normalization, imputation, smoothing, or feature extraction alter the representation of data and may directly influence downstream statistical results. QC, in contrast, is designed to operate **prior to such transformations** and serves a different purpose: to evaluate whether the raw data are sufficiently reliable to justify further processing.

QC therefore functions as a decision-support layer rather than a data-modifying layer. Its outputs consist of metrics, scores, flags, and summaries that describe data quality properties. These outputs inform explicit decisions about data inclusion, exclusion, or weighting, but do not themselves modify the data.

By positioning QC upstream of analysis-level preprocessing, *prepkit* prevents circular reasoning in which preprocessing choices implicitly define what is considered "good" data. This ordering ensures that quality assessment reflects properties of the observed data rather than artifacts introduced by downstream transformations.

Importantly, QC in *prepkit* is not intended to optimize data for predictive performance or model fit. Improving model performance by altering data representations is the responsibility of analysis-level preprocessing and modeling, not QC. Maintaining this boundary preserves the interpretability and auditability of quality-related decisions.

## QC Pipeline Overview

*prepkit* adopts a structured and sequential QC pipeline in which each stage has a clearly defined responsibility. The pipeline is designed to make quality-related assumptions explicit and to prevent implicit data modification prior to analysis.

The QC pipeline in *prepkit* consists of the following ordered stages:

1. **Raw data**
2. **QC-enabling preprocessing**
3. **QC assessment**
4. **QC decision** (optional but explicit)
5. **Analysis-level preprocessing**
6. **Post-processing validation**

Each stage in this pipeline is conceptually independent and must not be collapsed into adjacent steps.

QC-enabling preprocessing prepares data for quality assessment without altering their substantive meaning. QC assessment then evaluates observable quality properties and produces explicit metadata. QC decisions translate assessment results into concrete actions, such as filtering, weighting, or stratification, but are never performed automatically.

Only after QC decisions have been made does analysis-level preprocessing occur. This ordering ensures that transformations intended to support modeling or inference do not retroactively define what is considered acceptable data quality.

Finally, post-processing validation serves as a consistency check to verify that downstream transformations have not violated key analytical assumptions. This step does not redefine data quality and does not replace QC assessment.

By enforcing a fixed processing order and clear stage boundaries, *prepkit* provides a QC framework that is explicit, auditable, and resistant to unintended data leakage.

## QC-Enabling Preprocessing

QC-enabling preprocessing refers to transformations that make quality-related properties observable or computable without altering the substantive meaning of the data.

The purpose of this stage is not to improve data quality, but to prepare data for quality assessment. As such, QC-enabling preprocessing must preserve the original data values or their interpretability.

A practical operational rule in *prepkit* is that QC-enabling preprocessing **must not overwrite raw observations**. It may derive auxiliary objects (e.g., window indices, expected time grids, missingness masks, robust summaries) and attach them as metadata or as additional derived columns, but should not replace the original signal values. This rule makes QC auditable and reduces ambiguity when "reversibility" is technically possible but operationally hard to guarantee.

Typical QC-enabling preprocessing operations include:

- Alignment to expected observation grids **without numerical imputation**
- Explicit representation of missingness (e.g., masks, run-length summaries)
- Derivation of auxiliary quantities for QC metrics (e.g., per-window summaries)
- Robust transformations used solely for comparability (e.g., unit conversion with recorded parameters)

Operations that modify data values with the intent of improving downstream analysis, such as imputation, smoothing, interpolation, or normalization for modeling, are explicitly excluded from this stage.

### Boundary cases: grid alignment and resampling

Some operations (e.g., resampling or interpolation to a fixed grid) can be motivated by QC computation but are intrinsically information-discarding. In *prepkit*, such operations are treated as **analysis-level preprocessing** unless they are implemented as *non-destructive* augmentations (e.g., producing a separate resampled view while preserving the original observations and recording the mapping).

## QC Assessment

QC assessment is the stage at which observable data-quality properties are translated into explicit metrics, scores, or flags and returned as metadata.

The role of QC assessment in *prepkit* is to make data quality **evaluable, comparable, and auditable** across samples, time windows, or subjects. QC assessment does not modify data values and does not perform any form of automatic decision-making or data exclusion.

Typical QC assessment dimensions include:

- Coverage relative to expected observations
- Structure and extent of missingness
- Signal stability or plausibility
- Detection of flatline or saturation behavior
- Variance collapse or degeneracy
- Sampling irregularities (e.g., timestamp monotonicity, rate drift, duplicated times)

### Schema and metadata QC (structural QC)

A frequent source of failure in real-world datasets is not the signal itself but the data structure and metadata consistency. *prepkit* therefore treats the following as first-class QC assessment targets:

- Timestamp integrity (monotonicity, duplication, discontinuities)
- Time zone and day-boundary consistency
- Unit/scale plausibility (e.g., g vs m/s^2)
- Channel presence and naming consistency
- Device orientation/axis definitions and availability of required metadata
- Participant/session identifiers and join keys

These checks produce diagnostics and flags, but they do not "repair" the data automatically.

### QC metrics must be outcome-agnostic

QC assessment metrics must not depend on downstream outcomes, labels, or model objectives. They describe properties of the observed data, not their usefulness for a specific predictive task.

The M-score is an example of a QC assessment metric in *prepkit*. When used in this context, it quantifies structural characteristics of the data without introducing assumptions related to downstream models, optimization objectives, or analytical performance. Inputs to such metrics should be restricted to raw data and QC-enabling derived objects, not to representations produced by analysis-level preprocessing.

## QC Decision

QC decision is the stage at which QC assessment outputs are translated into explicit, user-invoked actions that affect downstream processing.

In *prepkit*, QC decisions are intentionally separated from QC assessment. While QC assessment produces metrics, scores, flags, and diagnostics, QC decision determines how, or whether, those outputs influence subsequent analysis.

Typical QC decisions include:

- Excluding observations or time windows
- Assigning weights based on quality indicators
- Stratifying data by quality tiers

QC decisions are never performed automatically. They must be explicitly invoked by the user and must be based on clearly defined criteria derived from QC assessment outputs.

### Policy templates and explainability

To support reproducibility without enforcing one "correct" rule set, *prepkit* may provide optional policy templates (e.g., strict / balanced / lenient). When a policy is applied, the decision output should record:

- The policy name and version
- Thresholds and parameters used
- The affected unit (subject/day/window/etc.)
- Explicit reasons (human-readable) for each exclusion/weighting action

This makes QC decisions inspectable and suitable for reporting in methods sections.

## Analysis-Level Preprocessing

Analysis-level preprocessing refers to transformations that directly modify the data representation for the purpose of statistical analysis or modeling.

Unlike QC-related stages, analysis-level preprocessing is explicitly allowed to alter data values and may influence downstream analytical results. These transformations are therefore placed after QC decisions to avoid circular reasoning and unintended data leakage.

Typical analysis-level preprocessing operations include:

- Normalization or scaling for modeling
- Imputation of missing values
- Smoothing or denoising
- Resampling/interpolation that replaces observed values
- Feature extraction and aggregation

In *prepkit*, analysis-level preprocessing is treated as a distinct stage with clearly declared intent. Functions in this category are not considered quality control tools, even if they reduce noise or highlight anomalous behavior.

By separating analysis-level preprocessing from QC, *prepkit* ensures that data quality assessment reflects properties of the observed data rather than artifacts introduced by analysis-oriented transformations.

## Post-Processing Validation

Post-processing validation is the stage at which the effects of analysis-level preprocessing are examined to ensure that key analytical assumptions have not been violated.

Unlike QC assessment, post-processing validation does not redefine raw data quality. Instead, it serves as a consistency and sanity check on the outcomes of preprocessing steps that intentionally modify data values.

Typical post-processing validation activities include:

- Verifying that value ranges remain plausible
- Checking for unintended loss of variability
- Confirming that preprocessing did not introduce systematic artifacts
- Inspecting summary statistics before and after transformation

Post-processing validation may emit warnings or diagnostic flags indicating **processing-induced issues** (e.g., variance collapse after smoothing). However, these diagnostics are interpreted as properties of the *processing outcome*, not as retroactive statements about raw data quality.

Post-processing validation does not replace QC assessment and must not be used to justify retroactive changes to QC decisions. Its role is limited to identifying potential issues introduced by analysis-level preprocessing so that they can be addressed explicitly.

## Reversibility and Irreversibility

A central principle of the QC framework adopted in this project is the explicit distinction between **reversible** and **irreversible** operations in data preprocessing.

In practice, many preprocessing steps modify data in ways that cannot be undone, yet such transformations are often applied early in the analysis without being explicitly acknowledged. This obscures the boundary between *data representation* and *data alteration*, and makes it difficult to assess how sensitive downstream results are to preprocessing choices.

In this framework, reversibility is treated not as a technical detail, but as a methodological criterion.

### Reversible operations

Reversible operations are transformations for which the original data can be fully reconstructed, either exactly or up to numerical precision, given the transformation parameters. Typical examples include:

- Unit conversions (e.g., seconds to minutes)
- Axis reorientation or sign changes
- Re-indexing, reshaping, or reordering of observations
- Linear transformations with stored parameters

These operations do not alter the statistical information content of the data. They are therefore permitted within **QC-enabling preprocessing**, where the goal is to make the data *inspectable* rather than *interpreted*.

### Irreversible operations

Irreversible operations are transformations that discard information in a way that cannot be recovered. Examples include:

- Smoothing and filtering
- Interpolation of missing values
- Truncation or thresholding
- Normalization based on sample-dependent statistics
- Dimensionality reduction

Such operations introduce assumptions about the data-generating process and directly affect the distributional structure of the data. As a result, they belong strictly to **analysis-level preprocessing**, after QC assessment and decision have been completed.

### Why this distinction matters

By separating reversible and irreversible steps, the QC pipeline ensures that:

1. Data quality is assessed on representations that remain faithful to the original observations.
2. Decisions about excluding, flagging, or down-weighting data are made **before** information is lost.
3. The impact of irreversible preprocessing on analysis results can be explicitly evaluated.

This distinction also provides a clear audit trail: any irreversible transformation applied to the data must be justified in the context of a specific analytical objective, rather than being implicitly embedded in early preprocessing steps.

## Optional QC and User Responsibility

Quality Control (QC) in this framework is designed to be **explicit but optional**.

This means that while the framework provides structured tools and concepts for assessing data quality, it does not enforce QC as a mandatory step. Users retain full responsibility for deciding whether and how QC results are used in their analyses.

### Why QC is optional

In real-world research workflows, there are legitimate reasons to proceed without formal QC assessment, including:

- Exploratory analyses where strict data exclusion is undesirable
- Legacy pipelines where QC decisions have already been made implicitly
- Situations where downstream models are explicitly designed to handle noise or missingness

For these reasons, the framework avoids hard enforcement of QC rules and instead emphasizes transparency and documentation.

### Explicit responsibility of the user

Although QC is optional, bypassing QC is itself a methodological decision.

When QC assessment is skipped, the user implicitly assumes that:

- The data are of sufficient quality for the intended analysis
- Any data artifacts or irregularities will not materially affect the results
- The analytical conclusions are robust to potential data quality issues

The framework therefore treats the absence of QC assessment as an explicit user responsibility rather than a default or neutral choice.

### Separation of responsibility

The QC framework enforces a clear separation of responsibility:

- The framework provides tools for QC assessment and reporting
- The user decides how QC results influence data inclusion, weighting, or interpretation
- Downstream analyses remain accountable to the assumptions implied by these decisions

This separation ensures flexibility while preserving methodological clarity.

### Documentation and traceability

To support responsible use, the framework encourages users to record:

- Whether QC assessment was performed
- Which QC metrics or diagnostics were considered
- How QC outcomes influenced subsequent preprocessing or analysis steps

Such documentation enables reproducibility, facilitates peer review, and allows downstream users to understand the role that data quality considerations played in the analytical workflow.

## Design Principles Summary

The QC framework presented in this project is guided by a small set of explicit design principles. These principles are intended to remain stable over time, even as specific methods and implementations evolve.

1. **QC is assessment, not correction**  
   Quality Control is defined as the evaluation of data quality, not the modification of data. Any operation that alters the data belongs to preprocessing rather than QC assessment.

2. **Reversibility defines the QC boundary**  
   Reversible operations may be used to enable QC assessment, while irreversible operations must be deferred until after QC decisions are made. This distinction preserves the integrity of quality evaluation.

3. **QC is explicit but optional**  
   The framework provides structured QC tools without enforcing their use. Skipping QC is a conscious methodological choice and carries explicit responsibility.

4. **Algorithms and semantics are not interchangeable**  
   The same computational construct may serve different roles depending on context. Identical algorithms can be used for QC assessment or for data transformation, but their outputs and interpretations must remain distinct.

5. **Decisions must be separable from measurements**  
   QC metrics describe data properties; QC decisions act on those descriptions. Conflating the two obscures assumptions and undermines interpretability.

6. **Transparency outweighs automation**  
   Automated QC is valuable only insofar as its criteria and consequences are visible. The framework prioritizes traceability and explanation over opaque rule-based exclusion.

7. **Users retain analytical responsibility**  
   Final responsibility for data inclusion, preprocessing, and interpretation lies with the user. The framework exists to support, not replace, informed judgment.
